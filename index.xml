<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hello on Vaiva Vasiliauskaite</title>
    <link>https://vv2246.github.io/</link>
    <description>Recent content in Hello on Vaiva Vasiliauskaite</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 21 Mar 2024 14:41:16 +0200</lastBuildDate><atom:link href="https://vv2246.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Quantifying reliability and accuracy of machine learning models</title>
      <link>https://vv2246.github.io/notes/quantifying_reliability_ml/</link>
      <pubDate>Thu, 21 Mar 2024 14:41:16 +0200</pubDate>
      
      <guid>https://vv2246.github.io/notes/quantifying_reliability_ml/</guid>
      <description>Machine learning and Artificial Intelligence have been hot topics for as long as I have been doing research. However, more often than not, I see new papers focusing on prediction or classification accuracy, with less emphasis on understanding the generalization capabilities of models. More models, more data, more applications&amp;hellip; The innovation is driven by increased accuracy on benchmark datasets. While there is a lower bound for the amount of data, and the complexity of a model, that are necessary to learn the data generating function, perhaps the need for ever bigger datasets and models points to our lack of understanding of the basic principles about the systems that are studied through machine learning models.</description>
    </item>
    
    <item>
      <title>Artificial Neural Networks</title>
      <link>https://vv2246.github.io/primers/artificial_neural_networks/</link>
      <pubDate>Tue, 17 Oct 2023 16:05:16 +0200</pubDate>
      
      <guid>https://vv2246.github.io/primers/artificial_neural_networks/</guid>
      <description>Neural computing is a machine learning paradigm which aims to emulate neurological function and learning. “Artificial Neural Network” (ANN/NN) is constructed merely by appropriately connecting a group of adaptable nodes (“artificial neurons”). ANN is a model of neural computation, deduced from simplified units which mimic integration and activation properties of real neurons. The simplest type of an ANN, composed of a single layer of neurons is a perceptron. The simplest architecture (a network of neurons) is a directed acyclic graph, where neurons are arranged into layers and do not connect to one another intralayer.</description>
    </item>
    
    <item>
      <title>Different types of models</title>
      <link>https://vv2246.github.io/notes/types_of_models/</link>
      <pubDate>Wed, 18 Jan 2023 14:41:16 +0200</pubDate>
      
      <guid>https://vv2246.github.io/notes/types_of_models/</guid>
      <description>The overarching goal of science is to explain the world: the nature, the life, or the society. These explanations oftentimes come in the form of models. But there are different types of models, aimed at different goals. I wanted to draw a distinction between different types of models, and got inspired by two papers [1,2] in my search. This is a starter to built upon regarding the notion of modeling in science.</description>
    </item>
    
    <item>
      <title>Science Aesthetic</title>
      <link>https://vv2246.github.io/notes/science_aesthetic/</link>
      <pubDate>Wed, 30 Nov 2022 14:41:16 +0200</pubDate>
      
      <guid>https://vv2246.github.io/notes/science_aesthetic/</guid>
      <description>Aesthetics is a branch of philosophy that deals with the nature of: beauty, taste, art. Aesthetics seems subjective and therefore opposite to scientific, yet it does exist in science. In fact, scientific aesthetics is so pervasive that we expect nature to be elegant, simple and symmetric. Any empirically valid result may be deemed incomplete due to the lack of any of these qualities. A big part of science is admirable, both visually as well as mathematically.</description>
    </item>
    
    <item>
      <title>Time</title>
      <link>https://vv2246.github.io/primers/time/</link>
      <pubDate>Tue, 14 Jun 2022 14:41:16 +0200</pubDate>
      
      <guid>https://vv2246.github.io/primers/time/</guid>
      <description>Time in Physics Our universe is described using a notion of spacetime, in which events take place. These events are characterised by their spatial coordinates, as well as &amp;ldquo;location&amp;rdquo; in time, so spacetime is a special type of Metric space where one coordinate is time. The time coordinate is very special, as it is the only coordinate with which it is meaningful to define an ordering in the values given, the order of time[1].</description>
    </item>
    
    <item>
      <title>Attractors</title>
      <link>https://vv2246.github.io/primers/attractors/</link>
      <pubDate>Tue, 31 May 2022 14:41:16 +0200</pubDate>
      
      <guid>https://vv2246.github.io/primers/attractors/</guid>
      <description>The basin of attraction is the set of initial conditions from which the solutions converge asymptotically to a given attractor. Since the basin can include the points quite distant from the attracting set, the size of the basin, as a general rule, is not determined by the local properties of the attractor. . In dissipative maps and flows, it is delimited by the complex geometrical configuration of stable manifolds of unstable invariant sets, which can lead to fractal boundaries[1]</description>
    </item>
    
    <item>
      <title>Emergence</title>
      <link>https://vv2246.github.io/primers/emergece/</link>
      <pubDate>Sun, 22 May 2022 14:41:16 +0200</pubDate>
      
      <guid>https://vv2246.github.io/primers/emergece/</guid>
      <description>In the so-called sciences of complexity (e.g., non-linear dynamics, theoretical biology, complex adaptive systems, artificial life, artificial intelligence, cognitive science), &amp;ldquo;complex&amp;rdquo; phenomena, such as the appearance of life on Earth, the evolution of new species, or the structure of cognitive thought, are often considered as instances of some emergent higher-order structure that may be explained by the lower-level dynamics generating the collective behaviour or emergent property of the system in question.</description>
    </item>
    
    <item>
      <title>Perceptron</title>
      <link>https://vv2246.github.io/primers/perceptron/</link>
      <pubDate>Wed, 04 May 2022 14:41:16 +0200</pubDate>
      
      <guid>https://vv2246.github.io/primers/perceptron/</guid>
      <description>Perceptron is a type of artificial neural network. The perceptron is as good as a Classification algorithm. The perceptron works by finding a hyperplane that separates points with different labels in the training set, as long as they are linearly separable.
Consider a function $g(\textbf{x})\rightarrow {0,1}$ which is what we want to approximate. If the function is parameterised by a set of coefficients ${w_1,w_2, &amp;hellip;,w_n}=\textbf{w}$, then the function can be rewritten in the form of an inner product $\textbf{w} \cdot \textbf{x}$ .</description>
    </item>
    
    <item>
      <title>Probability theory</title>
      <link>https://vv2246.github.io/primers/probability_theory/</link>
      <pubDate>Thu, 28 Apr 2022 14:41:16 +0200</pubDate>
      
      <guid>https://vv2246.github.io/primers/probability_theory/</guid>
      <description>Random phenomena are observed by means of experiments. Each experiment results in an outcome $\omega$. The collection of all outcomes is a sample space $\Omega$. Any subset of $\Omega$ is an event.
The collection $\mathcal{F}$ of events to which a probability is assigned is not always identical to the collection of all subsets of $\Omega$. The requirement on $\mathcal{F}$ is that it should be a $\sigma$-field:
F contains the sample space Ω.</description>
    </item>
    
    <item>
      <title>Random Variable</title>
      <link>https://vv2246.github.io/primers/random_variable/</link>
      <pubDate>Thu, 28 Apr 2022 14:41:16 +0200</pubDate>
      
      <guid>https://vv2246.github.io/primers/random_variable/</guid>
      <description>Random variable Probability theory defines a probability space as a triple ${\Omega,X,p}$, where $\Omega$ is the space of all elementary events, $X$ are disjoint subsets of $\Omega$, called events and $p$ is a probability function that maps events in $X$ to the closed unit interval $p:X\rightarrow [0,1]$. A random variable $x:\Omega\rightarrow X$ maps from the sample space to events. The probability $p$ of a discrete random variable $x$ belongingto the event $x_i ∈ X$ as the result of a statistical process is: $p({ω ∈ Ω : x(ω) ∈ x_i}) = |x_i|/{|\Omega|}$ assuming there is a uniform probability of any elementary event $\omega$ occurring.</description>
    </item>
    
    <item>
      <title>Game of life</title>
      <link>https://vv2246.github.io/primers/primer_gameoflife/</link>
      <pubDate>Wed, 27 Apr 2022 14:41:16 +0200</pubDate>
      
      <guid>https://vv2246.github.io/primers/primer_gameoflife/</guid>
      <description>Conway’s Life is an example of zero-player game, whose evolution is determined by the initial state.
The game happens on a two-dimensional Cellular Automaton with a simple local update rule that can produce complex global behavior. In a Life configuration, cells in an $n × m$ grid can be either alive or dead (represented by $1$ or $0$ respectively). The life To determine the state of a given cell on the next step, Life considers the $3 × 3$ grid of neighbors around the cell.</description>
    </item>
    
    <item>
      <title>Fun</title>
      <link>https://vv2246.github.io/fun/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vv2246.github.io/fun/</guid>
      <description>My one-year-old Australian Shepherd Nevis. Ball forever
Exploring aargau castles
Hiking near schwyz
In stoos
Moonboard nevis
My best friend
Nevis happy
Nevis having monday morning blues
Pizol five lake hike
Us in baden
We on a hike
Some photographs of beautiful places. A bookshop in istanbul
Alpenglow on dom taeschhorn lenzspitze
Beautiful venice
Impressive bergschrund on weissmies</description>
    </item>
    
    <item>
      <title>Links &amp; Contacts</title>
      <link>https://vv2246.github.io/contacts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vv2246.github.io/contacts/</guid>
      <description>Click here to see my CV.
Email Email: vvasiliau@ethz.ch Email (personal): vasiliauskaite.vaiva@gmail.com University address Computational Social Science, ETH Zürich STD F 6 Stampfenbachstrasse 48 CH-8006 Zürich
Socials Google scholar Github Research gate Twitter LinkedIn </description>
    </item>
    
    <item>
      <title>Publications</title>
      <link>https://vv2246.github.io/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vv2246.github.io/publications/</guid>
      <description>List of my papers Vasiliauskaite, V., &amp;amp; Antulov-Fantulin, N. (2023). How accurate are neural approximations of complex network dynamics? link Gheorghiade, P., Vasiliauskaite, V., Diachenko, A., Price, H., Evans, T., &amp;amp; Rivers, R. (2023). Entropology: an information-theoretic approach to understanding archaeological data. Journal of Archaeological Method and Theory, 30(4), 1109-1141. link Vasiliauskaite, V., Hausladen, C.I. (2023). How Do Circadian Rhythms and Neural Synchrony Shape Networked Cooperation? Front. Phys. Sec. Social Physics 11-17 link M.</description>
    </item>
    
  </channel>
</rss>
